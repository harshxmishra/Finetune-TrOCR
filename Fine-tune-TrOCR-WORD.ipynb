{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fabfc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\trocr\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, default_data_collator, EncoderDecoderModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from datasets import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c4fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54e6ecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n",
      "Current device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Current device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0378b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_path ground_truth\n",
      "0  /home/mrunali/indic_ocr/trOCR/data/english_hw_...           it\n",
      "1  /home/mrunali/indic_ocr/trOCR/data/english_hw_...            a\n",
      "2  /home/mrunali/indic_ocr/trOCR/data/english_hw_...          and\n",
      "3  /home/mrunali/indic_ocr/trOCR/data/english_hw_...          but\n",
      "4  /home/mrunali/indic_ocr/trOCR/data/english_hw_...         that\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DATA _TrOCR\\OCR_clean.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206f4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "# we reset the indices to start from zero\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42aeb002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>JUSTINE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>MADEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>TIRMAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>EVA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>CLEMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68103</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>THIBAULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68104</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>LEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68105</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>BARACHET</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68106</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>SHAINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68107</th>\n",
       "      <td>/home/mrunali/indic_ocr/trOCR/data/english_hw_...</td>\n",
       "      <td>SCHWEIGHEISER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68108 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               file_path   ground_truth\n",
       "0      /home/mrunali/indic_ocr/trOCR/data/english_hw_...        JUSTINE\n",
       "1      /home/mrunali/indic_ocr/trOCR/data/english_hw_...          MADEN\n",
       "2      /home/mrunali/indic_ocr/trOCR/data/english_hw_...         TIRMAN\n",
       "3      /home/mrunali/indic_ocr/trOCR/data/english_hw_...            EVA\n",
       "4      /home/mrunali/indic_ocr/trOCR/data/english_hw_...        CLEMENT\n",
       "...                                                  ...            ...\n",
       "68103  /home/mrunali/indic_ocr/trOCR/data/english_hw_...       THIBAULT\n",
       "68104  /home/mrunali/indic_ocr/trOCR/data/english_hw_...            LEA\n",
       "68105  /home/mrunali/indic_ocr/trOCR/data/english_hw_...       BARACHET\n",
       "68106  /home/mrunali/indic_ocr/trOCR/data/english_hw_...         SHAINA\n",
       "68107  /home/mrunali/indic_ocr/trOCR/data/english_hw_...  SCHWEIGHEISER\n",
       "\n",
       "[68108 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b02871e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "        self.image_paths = self._collect_image_paths()\n",
    "\n",
    "    def _collect_image_paths(self):\n",
    "        image_paths = {}\n",
    "        for root, dirs, files in os.walk(self.root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(('.tif', '.jpg', '.png')):  # Add other image formats if needed\n",
    "                    image_paths[file] = os.path.join(root, file)\n",
    "        return image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get file name + text\n",
    "        full_file_path = self.df['TIF_File_Path'].iloc[idx]\n",
    "        text = self.df['Ground_Truth'].iloc[idx]\n",
    "        \n",
    "        # Ensure the text is of type str\n",
    "        text = str(text)\n",
    "\n",
    "        # Extract just the file name from the full path\n",
    "        file_name = os.path.basename(full_file_path)\n",
    "\n",
    "        # Prepare image (i.e., resize + normalize)\n",
    "        image_path = self.image_paths.get(file_name)\n",
    "\n",
    "        if image_path is None:\n",
    "            raise FileNotFoundError(f\"Image {file_name} not found in {self.root_dir}\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Resize image to expected size (example: 224x224), you can change it based on your model's requirement\n",
    "        image = image.resize((224, 224))\n",
    "        \n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # Add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(text, \n",
    "                                          padding=\"max_length\", \n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        \n",
    "#         print(\"labels:\",labels)\n",
    "#         print(\"labels type:\",type(labels))\n",
    "        # Important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0103f742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "# Usage\n",
    "train_dataset = CustomDataset(\n",
    "    root_dir=r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\words', \n",
    "    df=train_df, \n",
    "    processor=processor\n",
    ")\n",
    "eval_dataset = CustomDataset(\n",
    "    root_dir=r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\words', \n",
    "    df=test_df, \n",
    "    processor=processor\n",
    ")\n",
    "\n",
    "# Example to access the first item\n",
    "encoding = train_dataset[0]\n",
    "for k, v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b2041b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 8611\n",
      "Number of validation examples: 2153\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f07b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"microsoft/trocr-base-handwritten\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4054ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e76e972",
   "metadata": {},
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    fp16=True, \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=200,\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf13c20",
   "metadata": {},
   "source": [
    "from datasets import load_metric\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fc8493",
   "metadata": {},
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55148f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23fb89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "# Adjust training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,  # Reduced batch size\n",
    "    per_device_eval_batch_size=2,# Reduced batch size\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    output_dir=\"./\",\n",
    "    logging_steps=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=200,\n",
    ")\n",
    "\n",
    "# Clear GPU cache before starting\n",
    "# model = model.to(device)\n",
    "    \n",
    "\n",
    "# Instantiate the trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a6dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0cd741",
   "metadata": {},
   "source": [
    "INFERENCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c231192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\trocr\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\models--microsoft--trocr-base-stage1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to the directory where your model and processor checkpoints are saved\n",
    "MODEL_PATH= r\"C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\Trained_model\\checkpoint-1614/\"\n",
    "\n",
    "# Load the processor and model from the checkpoint\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38886b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f9359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: Nomacs\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=322x61>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    # Resize the image to match the input size expected by the model\n",
    "    # image = image.resize((224, 224))\n",
    "    return image\n",
    "\n",
    "# Path to the new image for inference\n",
    "image_path = r\"C:\\Users\\ASUS\\Downloads\\d03-112-01-00.png\"\n",
    "image = preprocess_image(image_path)\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "# Move pixel values to GPU if available\n",
    "pixel_values = pixel_values.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(pixel_values)\n",
    "\n",
    "# Move the predictions back to CPU if necessary\n",
    "pred_ids = outputs.cpu()\n",
    "\n",
    "# Decode the predictions\n",
    "predicted_text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Predicted Text:\", predicted_text)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dda0b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Path  Ground_Truth\n",
      "0  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...         short\n",
      "1  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...            of\n",
      "2  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...       wearing\n",
      "3  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...           the\n",
      "4  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...          same\n",
      "5  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...         which\n",
      "6  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...  contemplated\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the dictionary using raw strings\n",
    "data = {\n",
    "    'Path': [\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-00.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-01.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-02.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-03.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-04.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-07.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-01-01.png'\n",
    "    ],\n",
    "    'Ground_Truth': ['short', 'of', 'wearing', 'the', 'same', 'which', 'contemplated']\n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f364f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbaab9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\ASUS\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\cer\\46482e3826224451c26c9b51d8d193d38a4226daab693df497d2e397b623274e (last modified on Wed Jun 26 16:22:44 2024) since it couldn't be found locally at cer, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Text: sheet\n",
      "CER: 0.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the CER metric\n",
    "cer_metric = load_metric(\"cer\")\n",
    "\n",
    "def infer(image_path, ground_truth):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Process the image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Move the pixel values to the appropriate device\n",
    "    pixel_values = pixel_values.to(model.device)\n",
    "\n",
    "    # Generate the text\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Calculate CER\n",
    "    cer = cer_metric.compute(predictions=[generated_text], references=[ground_truth])\n",
    "\n",
    "    return generated_text, cer\n",
    "\n",
    "# Example usage\n",
    "image_path = r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DATA _TrOCR\\a05-004-00-00.png'\n",
    "ground_truth = \"short\"\n",
    "predicted_text, cer = infer(image_path, ground_truth)\n",
    "\n",
    "print(f\"Predicted Text: {predicted_text}\")\n",
    "print(f\"CER: {cer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc78d25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e236181c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Ground_Truth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\trocr\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Ground_Truth'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n\u001b[0;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 19\u001b[0m ground_truths \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGround_Truth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Perform inference on each image\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\trocr\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\trocr\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Ground_Truth'"
     ]
    }
   ],
   "source": [
    "def infer(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Process the image\n",
    "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "    # Move the pixel values to the appropriate device\n",
    "    pixel_values = pixel_values.to(model.device)\n",
    "\n",
    "    # Generate the text\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(pixel_values)\n",
    "        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "predictions = []\n",
    "ground_truths = df['Ground_Truth'].tolist()\n",
    "\n",
    "# Perform inference on each image\n",
    "for image_path in df['Path']:\n",
    "    predicted_text = infer(image_path)\n",
    "    predictions.append(predicted_text)\n",
    "\n",
    "# Calculate CER\n",
    "cer = cer_metric.compute(predictions=predictions, references=ground_truths)\n",
    "\n",
    "# Print results\n",
    "for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "    print(f\"Image {i+1}:\")\n",
    "    print(f\"  Ground Truth: {gt}\")\n",
    "    print(f\"  Prediction:  {pred}\")\n",
    "    \n",
    "    print(f\"CER: {cer}\")\n",
    "    df['cer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6ef7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = {\n",
    "    'Path': [\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-00.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-01.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-02.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-03.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-04.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-00-07.png',\n",
    "        r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05-004-01-01.png'\n",
    "    ],\n",
    "    'Actual_Text': ['short', 'of', 'wearing', 'the', 'same', 'which', 'contemplated'],\n",
    "    'Predicted_Text': ['sheet', 'of', 'working', 'the', 'same', 'which', 'computer'],\n",
    "    'CER': [0.2894736842105263] * 7 \n",
    "}\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\infer.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5e7d196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Actual_Text</th>\n",
       "      <th>Predicted_Text</th>\n",
       "      <th>CER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>short</td>\n",
       "      <td>sheet</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>wearing</td>\n",
       "      <td>working</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>same</td>\n",
       "      <td>same</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>which</td>\n",
       "      <td>which</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...</td>\n",
       "      <td>contemplated</td>\n",
       "      <td>computer</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Path   Actual_Text  \\\n",
       "0  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...         short   \n",
       "1  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...            of   \n",
       "2  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...       wearing   \n",
       "3  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...           the   \n",
       "4  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...          same   \n",
       "5  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...         which   \n",
       "6  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\a05...  contemplated   \n",
       "\n",
       "  Predicted_Text       CER  \n",
       "0          sheet  0.289474  \n",
       "1             of  0.289474  \n",
       "2        working  0.289474  \n",
       "3            the  0.289474  \n",
       "4           same  0.289474  \n",
       "5          which  0.289474  \n",
       "6       computer  0.289474  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = pd.read_csv(r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\infer.csv')\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7aa5c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv(r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DATA _TrOCR\\tif_filesAll2_cleaned.csv')\n",
    "df2 = pd.read_csv(r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DATA _TrOCR\\written_name_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c4f1d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TIF_FILE_PATH</th>\n",
       "      <th>Ground_Truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>BALTHAZAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>SIMON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>BENES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>LA LOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>DAPHNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330956</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>LENNY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330957</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>TIFFANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330958</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>COUTINHO DESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330959</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>MOURAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330960</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>HELOISE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330961 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            TIF_FILE_PATH   Ground_Truth\n",
       "0       C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...      BALTHAZAR\n",
       "1       C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...          SIMON\n",
       "2       C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...          BENES\n",
       "3       C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...        LA LOVE\n",
       "4       C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...         DAPHNE\n",
       "...                                                   ...            ...\n",
       "330956  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...          LENNY\n",
       "330957  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...        TIFFANY\n",
       "330958  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...  COUTINHO DESA\n",
       "330959  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...         MOURAD\n",
       "330960  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...        HELOISE\n",
       "\n",
       "[330961 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3510768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "File_paths=df1['TIF_File_Path']\n",
    "grount_truth=df1['Ground_Truth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b70bcebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "1         C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "2         C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "3         C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "4         C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "                                ...                        \n",
      "330956    C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "330957    C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "330958    C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "330959    C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "330960    C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...\n",
      "Name: TIF_FILE_PATH, Length: 330961, dtype: object\n"
     ]
    }
   ],
   "source": [
    "File_paths2=df2['TIF_FILE_PATH']\n",
    "grount_truth2=df2['Ground_Truth']\n",
    "\n",
    "print(File_paths2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d488766",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63eabed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file_paths=np.concatenate([File_paths,File_paths2])\n",
    "combined_ground_truth=np.concatenate([grount_truth,grount_truth2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b26b891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file 'combined_data.xlsx' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'file_path': combined_file_paths,\n",
    "    'ground_truth': combined_ground_truth\n",
    "}\n",
    "\n",
    "combined_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "combined_df.to_excel('combined_excel_files.xlsx', index=False)\n",
    "\n",
    "print(\"Excel file 'combined_data.xlsx' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b40aa9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             file_path ground_truth\n",
       "614  C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...           it\n",
       "615  C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...            a\n",
       "616  C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...          and\n",
       "617  C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...          but\n",
       "618  C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...         that"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\combined_excel_files.xlsx')\n",
    "data = data.iloc[614:]\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511d5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\Train_Data.csv'\n",
    "data.to_csv(new_path , index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb284ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341106</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>LENNY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341107</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>TIFFANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341108</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>COUTINHO DESA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341109</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>MOURAD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341110</th>\n",
       "      <td>C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...</td>\n",
       "      <td>HELOISE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>341111 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                file_path   ground_truth\n",
       "0       C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...             it\n",
       "1       C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...              a\n",
       "2       C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...            and\n",
       "3       C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...            but\n",
       "4       C:\\Users\\ASUS\\OneDrive\\Desktop\\EnglishDataSetW...           that\n",
       "...                                                   ...            ...\n",
       "341106  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...          LENNY\n",
       "341107  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...        TIFFANY\n",
       "341108  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...  COUTINHO DESA\n",
       "341109  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...         MOURAD\n",
       "341110  C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\DAT...        HELOISE\n",
       "\n",
       "[341111 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\ASUS\\OneDrive\\Desktop\\TROCR_MODEL\\Train_Data.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
